% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}\pagecolor[RGB]{28,30,38} \color[RGB]{213,216,218}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Markov Process},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{physics}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{esint}
\usepackage[ruled,vlined]{algorithm2e}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{definition*}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\Arg}{\mathop{\mathrm{Arg}}}
\newcommand{\hess}{\mathop{\mathrm{Hess}}}

% the redefinition for the missing \setminus must be delayed
\AtBeginDocument{\renewcommand{\setminus}{\mathbin{\backslash}}}

\title{Markov Process}
\author{Kexing Ying}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\section{Introduction and Review}

We will in this course assume the following notation:
\begin{itemize}
  \item \((\Omega, \mathcal{F}, \mathbb{P})\) is a probability space;
  \item \(\mathcal{X}\) is a Polish space, i.e. a separable, completely 
    metrizable, topological space;
  \item \(\mathcal{B}(\mathcal{X})\) is the Borel \(\sigma\)-algebra of \(\mathcal{X}\).
\end{itemize}

\begin{definition}[Stochastic Process]
  A stochastic process \((x_n)_{n \in I}\) is a collection of random variables. 
  In the case that \(I = \mathbb{N}\) or \(\mathbb{Z}\), we say that the 
  stochastic process is discrete time. On the other hand if \(I = \mathbb{R}_{\ge 0}\) 
  or \([0, 1] \subseteq \mathbb{R}\), then we say the process is continuous time.  
\end{definition}

We recall some definitions from elementary probability theory. 

\begin{definition}[Random Variable]
  A random variable \(x : \Omega \to \mathcal{X}\) is simply a measurable function.
\end{definition}

\begin{definition}[Probability Distribution]
  Given a random variable \(x : \Omega \to \mathcal{X}\), the probability 
  distribution of \(x\), denoted by \(\mathcal{L}(x)\) is the push-forward 
  measure of \(\mathbb{P}\) along \(x\), i.e. 
  \[\mathcal{L}(x) = x_* \mathbb{P} : A \in \mathcal{F} \mapsto \mathbb{P}(x^{-1}(A)).\]
\end{definition}

\begin{proposition}
  Let \(x : \Omega \to \mathcal{X}\) be a random variable where \(\mathcal{X}\) 
  is countable, then
  \[\mathcal{L}(x) = \sum_{i \in X} \mathbb{P}(x = i) \delta_i := 
    \sum_{i \in X} x_* \mathbb{P}(\{i\}) \delta_i\]
  where \(\delta_i\) is the Dirac measure concentrated at \(i\).
\end{proposition}
\begin{proof}
  Let \(A \subseteq X\), then 
  \[\mathcal{L}(x)(A) = \sum_{i \in A} \mathcal{L}(x)(\{i\}) = 
    \sum_{i \in X} \mathcal{L}(x)(\{i\})\delta_i(A) = 
    \sum_{i \in X} x_* \mathbb{P}(\{i\}) \delta_i(A),\]
  as required.
\end{proof}

\begin{definition}[Independence]
  Given random variables \(x_1, \cdots, x_n\), we say \(x_1, \cdots, x_n\) are 
  independent if 
  \[\mathcal{L}((x_1, \cdots, x_n)) = \bigotimes_{i = 1}^n \mathcal{L}(x_i),\]
  where \(\otimes\) denotes the product measure.
\end{definition}

As the name suggests, we will in this course mostly focus on a class of stochastic 
processes known as Markov processes. These are processes in which given information 
about the process at the present time, its future is independent from its 
history. In particular, if \((x_n)\) is a Markov process, given its value at 
\(x_k\), the value of \(x_j\) is independent of the values of \(x_i\) for all 
\(i < k < j\).

\begin{definition}[Invariant Probability Measure]
  A probability measure \(\pi\) is said to be an invariant probability measure 
  or an invariant distribution of a Markov process \((x_n)_{n \in I}\) if for all 
  \(n \in I\), we have \(\pi = \mathcal{L}(x_n)\).
\end{definition}

A Markov chain started from an invariant distribution does is called a 
stationary Markov process as its distribution do not evolve and we say that the 
chain is in equilibrium.

In this course we will study the behaviour of the distribution of Markov processes. 
In particular, we ask
\begin{itemize}
  \item does there exists an invariant measure? If so, is it unique?
  \item how does the distribution evolve over time?
  \item does \(\mathcal{L}(x_n)\) converge as \(n \to \infty\) 
    (convergence in distribution)?
\end{itemize}

\newpage
\section{Markov Property}

Let us now consider the Markov property in a more formal context.

\subsection{Filtration and Simple Markov Property}

In formation and filtration is an important notion not only for Markov processes 
but for stochastic processes in general. 

In formally, the information of a random 
variable \(x\) is the collection of all possible events, i.e. the sigma algebra 
generated by \(x\), 
\[\sigma(x) = \sigma(\{x^{-1}(A) \mid A \in \mathcal{B}(\mathcal{X})\}).\]
In the case of a stochastic process \((x_n)\), the information on the process up 
to time \(n\) is the \(\sigma\)-algebra generated by \(x_0, \cdots, x_n\), 
i.e. \(\sigma(x_0, \cdots, x_n)\).

With this in mind, we see that the notion of possible events evolving in time 
is naturally described by a sequence of increasing \(\sigma\)-algebras. 
We call such a sequence a filtration.

\begin{definition}[Filtration]
  A filtration is a sequence \((\mathcal{F}_n)\) of 
  increasing sub-\(\sigma\)-algebras of \(\mathcal{F}\). 
\end{definition}

\begin{definition}[Adapted]
  A stochastic process \((x_n)\) is adapted to the filtration \((\mathcal{F}_n)\) if 
  for all \(n\), \(x_n\) is \(\mathcal{F}_n\)-measurable.
\end{definition}

\begin{definition}[Natural Filtration]
  Given a stochastic process \((x_n)\), the natural filtration \((\mathcal{F}^x_n)\) 
  for \((x_n)\) is 
  \[\mathcal{F}^x_n := \sigma(x_0 ,\cdots, x_n).\]
  We note that by definition, a stochastic process is always adapted to its natural 
  filtration.
\end{definition}

Recalling the definition of conditional expectation, we introduce the following 
notations. 

\begin{definition}[Conditional Probability]
  Given a \(\sigma\)-algebra \(\mathcal{G} \subseteq \mathcal{F}\) and a random 
  variable \(x\), we define the conditional probability of \(x\) with respect 
  to \(\mathcal{G}\) to be 
  \[\mathbb{P}(x \in A \mid \mathcal{G}) := \mathbb{E}(\mathbf{1}_A(X) \mid \mathcal{G}),\]
  for all \(A \in \mathcal{B}(\mathcal{X})\) where \(\mathbf{1}_A\) is the indicator 
  function of \(A\). 

  Furthermore, given random variables \(x_0, \cdots, x_n\), we denote 
  \[\mathbb{P}(x \in A \mid x_0, \cdots, x_n) := \mathbb{P}(x \in A \mid \sigma(x_0, \cdots, x_n)).\]
\end{definition}

\begin{definition}[Simple Markov Property]
  A stochastic process \((x_n)\) with state space \(\mathcal{X}\) is said to have the 
  simple Markov property if for any \(A \in \mathcal{B}(\mathcal{X})\) 
  and \(n \ge 0\), we have 
  \[\mathbb{P}(x_{n + 1} \in A \mid x_0, \cdots, x_n) = \mathbb{P}(x_{n + 1} \in A \mid x_n),\]
  almost surely.

  Unfolding the notation, the simple Markov property states that 
  \[\mathbb{E}(\mathbf{1}_A(x_{n + 1}) \mid \mathcal{F}^x_n) = 
    \mathbb{E}(\mathbf{1}_A(x_{n + 1}) \mid \sigma(x_n)).\]

  We call a stochastic process which has the simple Markov property a Markov 
  process and we call \(\mathcal{L}(x_0)\) the initial distribution. Furthermore, 
  if the Markov process is discrete, we call it a Markov chain. 
\end{definition}

The definition of the simple Markov property can be generalized to continuous 
stochastic processes by taking the property to be 
\(\mathbb{E}(\mathbf{1}_A(x_t) \mid \mathcal{F}^x_s) = 
\mathbb{E}(\mathbf{1}_A(x_t) \mid \sigma(x_s))\) for all \(s \le t\).

In the case that \(\mathcal{X} = \mathbb{N}\), the simple Markov property is 
equivalent to the statement that 
\[\mathbb{P}(x_{n + 1} = j \mid x_0 = i_0, \cdots, x_n = i_n) = 
  \mathbb{P}(x_{n + 1} = j \mid x_n = i_n),\]
almost surely for every \(n\) where \(i_0, \cdots, i_n \in \mathcal{X} = \mathbb{N}\)
\[\mathbb{P}(x_0 = i_0, \cdots, x_n = i_n) > 0.\]

\begin{lemma}
  Let \(\mathcal{G} \subseteq \mathcal{F}\), \(X : \Omega \to \mathcal{X}, 
  Y : \Omega \to \mathcal{Y}\) be random variables such that \(X\) is 
  \(\mathcal{G}\)-measurable, \(Y\) is independent of \(\mathcal{G}\). 
  Then, if \(\phi : \mathcal{X} \times \mathcal{Y} \to \mathbb{R}\) is measurable 
  such that \(\phi(X, Y) \in L^1\), we have 
  \[\mathbb{E}(\phi(X, Y) \mid \mathcal{G})(\omega) = \mathbb{E}_Y(\phi(X(\omega), Y))\]
  almost surely. 
\end{lemma}
\begin{proof}
  Exercise.
\end{proof}

\begin{proposition}
  Let \(\xi_1, \xi_2, \cdots\) be a sequence of independent random variables with 
  state space \(\mathcal{Y}\) and is independent with respect to 
  \(x_0 : \Omega \to \mathcal{X}\). Then, if \(F : \mathcal{X} \times \mathcal{Y} \to \mathcal{X}\) 
  is a measurable function, we may define the stochastic process 
  \[x_{n + 1} = F(x_n, \xi_{n + 1}).\]
  \((x_n)\) is a Markov process. 
\end{proposition}
\begin{proof}
  Let \(A \in \mathcal{B}(\mathcal{X})\). Then, 
  \[\begin{split}
    \mathbb{E}(\mathbf{1}_A(x_{n + 1}) \mid x_0, \cdots, x_n) 
    & = \mathbb{E}(\mathbf{1}_A(F(x_n, \xi_{n + 1}) \mid x_0, \cdots, x_n) \\
    & = \omega \mapsto \mathbb{E}(\mathbf{1}_A(F(x_n(\omega), \xi_{n + 1})),
  \end{split}\]
  where the second equality follows by the above lemma (setting \(\phi = \mathbf{1}_A \circ F\) 
  and observing that \(x_n\) is \(\sigma(x_0, \cdots, x_n)\)-measurable and 
  \(\xi_{n + 1}\) is independent of \(\sigma(x_0, \cdots, x_n)\)). Similarly, 
  \[\begin{split}
    \mathbb{E}(\mathbf{1}_A(x_{n + 1}) \mid x_n) 
    & = \mathbb{E}(\mathbf{1}_A(F(x_n, \xi_{n + 1}) \mid x_n) \\
    & = \omega \mapsto \mathbb{E}(\mathbf{1}_A(F(x_n(\omega), \xi_{n + 1})),
  \end{split}\]
  we have \(\mathbb{E}(\mathbf{1}_A(x_{n + 1}) \mid \mathcal{F}^x_n) = 
  \mathbb{E}(\mathbf{1}_A(x_{n + 1}) \mid \sigma(x_n))\) as required.
\end{proof}

\subsection{Markov Property}

So far we have looked at the simple Markov property in which we have taken the 
filtration to be the natural filtration of the process. However, in the case 
that we are looking at multiple processes, we would like to consider a larger 
filtration such that each process is adapted. This motivates the general definition 
for the Markov property.

\begin{definition}
  Let \((\mathcal{F}_t)_{t \in I}\) be a filtration indexed by the set \(I\) on 
  the measurable space \((\Omega, \mathcal{F})\). A stochastic process \((x_t)_{t \in I}\) 
  on \(\mathcal{X}\) is a Markov process with respect to \(\mathcal{F}_t\) if it 
  is adapted to \(\mathcal{F}_t\) and 
  \[\mathbb{P}(x_t \in A \mid \mathcal{F}_s) = \mathbb{P}(x_t \in A \mid x_s)\]
  almost surely for all \(s, t \in I\), \(t > s\) and \(A \in \mathcal{B}(\mathcal{X})\).

  Again, unfolding the notation, the above statement says 
  \[\mathbb{E}(\mathbf{1}_A(x_t) \mid \mathcal{F}_s) = \mathbb{E}(\mathbf{1}_A(x_t) \mid \sigma(x_s))\]
  almost surely.
\end{definition}

\begin{proposition}
  If \((x_t)\) is a Markov process with respect to the filtration \((\mathcal{F}_t)\), 
  then it is also a Markov process with respect to its natural filtration \((\mathcal{F}_t^x)\).
\end{proposition}
\begin{proof}
  Recalling that \(\mathcal{F}_t^x \subseteq \mathcal{F}_t\) for all \(t\),
  by the tower property of the conditional expectation, we have 
  \[\begin{split}
    \mathbb{P}(x_{t + s} \in A \mid \mathcal{F}_s^x) & = 
    \mathbb{E}(\mathbf{1}_A(x_{t + s}) \mid \mathcal{F}_s^x) \\
    & = \mathbb{E}(\mathbb{E}(\mathbf{1}_A(x_{t + s}) \mid \mathcal{F}_s) \mid \mathcal{F}_s^x)\\
    & = \mathbb{E}(\mathbb{E}(\mathbf{1}_A(x_{t + s}) \mid \sigma(x_s)) \mid \mathcal{F}_s^x),
  \end{split}\]  
  where the equalities denotes equal a.e. Thus, as \(\mathbb{E}(\mathbf{1}_A(x_{t + s}) \mid \sigma(x_s))\) 
  is \(\sigma(x_s)\)-measurable, and thus \(\mathcal{F}_s^x\)-measurable (since 
  \(\sigma(x_s) \subseteq \sigma(x_r \mid r \le s) = \mathcal{F}_s^x)\)), we have 
  \[\mathbb{E}(\mathbb{E}(\mathbf{1}_A(x_{t + s}) \mid \sigma(x_s)) \mid \mathcal{F}_s^x) = 
  \mathbb{E}(\mathbf{1}_A(x_{t + s}) \mid \sigma(x_s))\]
  implying that the Markov property is satisfied.
\end{proof}

\begin{theorem}
  If \((x_t)\) is a Markov process with respect to the filtration \((\mathcal{F}_t)\), 
  then 
  \[\mathbb{E}(f(x_t) \mid \mathcal{F}_s) = \mathbb{E}(f(x_t) \mid \sigma(x_s))\]
  almost surely for any \(f : \mathcal{X} \to \mathbb{R}\) bounded and measurable. 
  In particular, this property is equivalent to the Markov property by choosing 
  \(f = \mathbf{1}_A\) for all \(A \in \mathcal{B}(\mathcal{X})\).
\end{theorem}
\begin{proof}
  By linearity, the property holds for simple functions. Furthermore, by the 
  conditional monotone convergence theorem, the property holds for any non-negative 
  bounded measurable functions. Finally, for arbitrary bounded measurable functions 
  \(f\), the result follows by taking \(f = f^+ - f^-\) and applying the non-negative 
  case.
\end{proof}

\begin{proposition}
  Let \(C \in \mathcal{F}_s\) and suppose \(\mathcal{B}(\mathcal{X}) = 
  \sigma(\mathcal{D})\) where 
  \(\mathcal{D}\) is a \(\pi\)-system (i.e. non-empty and closed under finite intersections), 
  then, if 
  \[\mathbb{E}(\mathbf{1}_A(x_{t + s}) \mathbf{1}_C) = 
    \mathbb{E}(\mathbb{P}(x_{t + s} \in A \mid x_s) \mathbf{1}_C)\] 
  holds for any \(A \in \mathcal{D}\), it holds for any \(A \in \mathcal{B}(\mathcal{X})\).
\end{proposition}
\begin{proof}
  Let \(\mathcal{A}\) be the set of Borel sets which the equation holds. Then, 
  by definition \(\mathcal{D} \subseteq \mathcal{A}\) and so, it suffices to show 
  \(\mathcal{A}\) is a \(\lambda\)-system (i.e. \(\mathcal{A}\) contains \(\mathcal{X}\), 
  closed under complements and closed under countable unions of increasing sets). 
  Indeed, Dynkin's \(\pi - \lambda\) theorem states that if \(\mathcal{D}\) is 
  a \(\pi\)-system, \(\mathcal{A}\) is a \(\lambda\)-system and 
  \(\mathcal{D} \subseteq \mathcal{A}\), then \(\sigma(\mathcal{D}) \subseteq \mathcal{A}\).

  Clearly \(\mathcal{X} \in \mathcal{A}\) since 
  \[\mathbb{E}(\mathbf{1}_{\mathcal{X}}(x_{t + s}) \mathbf{1}_C) = 
    \mathbb{E}(\mathbf{1}_C) = 
    \mathbb{E}(\mathbb{P}(x_{t + s} \in \mathcal{X} \mid x_s) \mathbf{1}_C).\]
  Suppose now \(A \in \mathcal{A}\). Then, the property holds as 
  \(\mathbf{1}_{A^c} = 1 - \mathbf{1}_A\) and so, the result follows by linearity.
  Finally, if \((A_n) \subseteq \mathcal{A}\) is increasing. Then by the 
  monotone convergence theorem for conditional expectations, it follows that 
  \(\bigcup A_n \in \mathcal{A}\) and hence, \(\mathcal{A}\) is a \(\lambda\)-system 
  as required.
\end{proof}

\begin{proposition}
  Suppose \(\mathbb{E}(f(x_{n + 1}) \mid x_0, \cdots, x_n) = \mathbb{E}(f(x_{n + 1}) \mid x_n)\)
  for any bounded measurable \(f\). Then, if we have a sequence 
  \[0 \le t_1 < t_2 < \cdots < t_{m - 1} < t_m = n - 1,\]
  where \(n > 1, t_i \in \mathbb{N}\), for any bounded measurable functions \(f, h\), 
  we have 
  \[\mathbb{E}(f(x_{n + 1})h(x_n) \mid x_{t_1}, \cdots, x_{t_m}) = 
    \mathbb{E}(f(x_{n + 1})h(x_n) \mid x_{n - 1}).\]
\end{proposition}
\begin{proof}
  Exercise.
\end{proof}

As we will often use the bounded measurable functions, let us denote the set 
of bounded measurable functions \(f : \mathcal{X} \to \mathbb{R}\) by 
\(\mathcal{B}_b(\mathcal{X})\).

\begin{lemma}
  Let \(X, Y \in L_1\) and \(\mathcal{G} \subseteq \mathcal{F}\). Then, if \(X\) 
  is \(\mathcal{G}\)-measurable and \(XY \in L_1\), we have
  \[\mathbb{E}(XY \mid \mathcal{G}) = X\mathbb{E}(Y \mid \mathcal{G}).\]
  We call this property ``taking out what is known''.
\end{lemma}
\begin{proof}
  See problem sheet 1.
\end{proof}

\begin{theorem}
  Given a stochastic process \((x_n)\) and indices \(l < m < n\), TFAE.
  \begin{itemize}
    \item For any \(f \in \mathcal{B}_b(\mathcal{X})\), 
      \[\mathbb{E}(f(x_n) \mid x_l, x_m) = \mathbb{E}(f(x_n) \mid x_m).\]
    \item For any \(g \in \mathcal{B}_b(\mathcal{X})\), 
      \[\mathbb{E}(g(x_l) \mid x_m, x_n) = \mathbb{E}(g(x_l) \mid x_m).\]
    \item For any \(f, g \in \mathcal{B}_b(\mathcal{X})\), 
      \[\mathbb{E}(f(x_n)g(x_l) \mid x_m) = \mathbb{E}(f(x_n) \mid x_m) \mathbb{E}(g(x_l) \mid x_m).\]
      That is to say, given now, the past is independent of the future.
  \end{itemize}
\end{theorem}
\begin{proof}
  Suppose the first statement holds, we will prove the third property. 
  Let \(f, g \in \mathcal{B}_b(\mathcal{X})\), then by the tower law and the above lemma, 
  we have 
  \[\begin{split}
    \mathbb{E}(f(x_n)g(x_l) \mid x_m) & = \mathbb{E}(\mathbb{E}(f(x_n)g(x_l) \mid x_m, x_l) \mid x_m)\\
    & = \mathbb{E}(g(x_l) \mathbb{E}(f(x_n) \mid x_m, x_l) \mid x_m)\\
    & = \mathbb{E}(g(x_l) \mathbb{E}(f(x_n) \mid x_m) \mid x_m)\\
    & = \mathbb{E}(f(x_n) \mid x_m)\mathbb{E}(g(x_l) \mid x_m)
  \end{split}\]
  which is exactly the third property.

  On the other hand, if the third property holds, for any \(g, h \in \mathcal{B}_b(\mathcal{X})\), 
  we have 
  \[\begin{split}
    \mathbb{E}(f(x_n)h(x_m)g(x_l)) & = \mathbb{E}(\mathbb{E}(f(x_n)g(x_l) \mid x_m)) h(x_m))\\
    & = \mathbb{E}(\mathbb{E}(f(x_n) \mid x_m)) \mathbb{E}(g(x_l) \mid x_m) h(x_m))\\
    & = \mathbb{E}(\mathbb{E}(\mathbb{E}(f(x_n) \mid x_m)g(x_l)h(x_m) \mid x_m))\\
    & = \mathbb{E}(\mathbb{E}(f(x_n) \mid x_m)g(x_l)h(x_m))
  \end{split}\]
  where the last equality is due to the law of total expectation. Now, by considering 
  this equality implies that, for all \(A = A_1 \cap A_2\) where 
  \(A_1 \in \sigma(x_l), A_2 \in \sigma(x_m)\), by choosing \(g = \mathbf{1}_{x_l^{-1}(A_1)}\) 
  and \(h = \mathbf{1}_{x_m^{-1}(A_2)}\), we have
  \[\int_A f(x_n) \dd \mathbb{P} = \int_A \mathbb{E}(f(x_n) \mid x_m) \dd \mathbb{P},\]
  and so, \(\mathbb{E}(f(x_n) \mid x_m) = \mathbb{E}(f(x_n) \mid x_l, x_m)\) almost 
  surely. Hence, the first and third property are equivalent. Similarly, one 
  can should that the second property is equivalent to the third property and 
  hence the equivalence.
\end{proof}

\begin{proposition}
  A stochastic process \((x_n)\) is a Markov process if and only if one of the following 
  conditions holds:
  \begin{itemize}
    \item for any \(f_i \in \mathcal{B}_b(\mathcal{X})\),
    \[\mathbb{E}\left(\prod_{i = 1}^n f_i(x_i)\right) = \mathbb{E}\left(\prod_{i = 1}^{n - 1} f_i(x_i) 
      \mathbb{E}(f_n(x_n) \mid x_{n - 1})\right).\]
    \item for any \(A_i \in \mathcal{B}(\mathcal{X})\), 
    \[\mathbb{P}(x_0 \in A_0, \cdots, x_n \in A_n) = 
      \int_{\bigcap_{i = 0}^{n - 1}\{x_i \in A_i\}} \mathbb{P}(x_n \in A_n \mid x_{n - 1}) \dd \mathbb{P}.\]
  \end{itemize}
\end{proposition}
\begin{proof}
  We note that by choosing \(f_i = \mathbf{1}_{A_i}\), the first condition implies 
  the second. On the other hand, the reverse implication follows by the standard routine 
  of proving it for simple function and using monotone convergence. Thus, it 
  suffices to establish an equivalence between the first condition 
  and the Markov property. This is left as an exercise.
\end{proof}

\subsection{Gaussian Measure and Gaussian Process}

As one of the most important distributions in probability theory, let us in this 
short section introduce the Gaussian measure which we will again encounter later 
on with this course.

\begin{definition}[Gaussian Measure]
  A measure \(\mu\) on \(\mathbb{R}^n\) is Gaussian if there exists a non-negative 
  definite symmetric matrix \(K\) and \(m \in \mathbb{R}^n\) such that the Fourier 
  transform of \(\mu\) is
  \[\int_{\mathbb{R}^n} e^{i\langle\lambda, x\rangle} \mu(\dd x) = 
    e^{i \langle \lambda, m \rangle - \frac{1}{2} \langle K \lambda, \lambda \rangle}\]
  for any \(\lambda \in \mathbb{R}^n\). We call the matrix \(K\) the covariance 
  of \(\mu\) and \(m\) its mean.

  We remark that if \(X\) is a random variable with distribution \(\mu\), then 
  the Fourier transform of \(\mu\) is simply the characteristic function of 
  \(X\), \(\mathbb{E}(e^{i\langle\lambda, X\rangle})\).
\end{definition}

\begin{proposition}
  If \(\mu\) is a Gaussian measure with covariance \(K\) and mean \(m\) 
  is absolutely continuous with respect to the Lebesgue measure if and only if 
  \(K\) is non-degenerate. In this case, for all \(A \in \mathcal{B}(\mathbb{R}^n)\),
  \[\mu(A) = \int_A \frac{1}{\sqrt{(2\pi)^n \det K}} 
    e^{-\frac{1}{2}\langle K^{-1}(x - m), x - m \rangle} \lambda(\dd x).\]
\end{proposition}

We observe that if \(X\) is a random variable with Gaussian distribution \(\mu\), 
then as one might expect, \(\mathbb{E}(X) = m\) and 
\[\text{Cov}(X_i, X_j) := \mathbb{E}(X_i - m_i)(X_j - m_j) = K_{ij}.\]
For this reason, we call \(K\) the covariance operator.

\begin{theorem}
  If \(X\) is a Gaussian random variable (i.e. its distribution is Gaussian) 
  on \(\mathbb{R}^d\) with covariance \(K\). Then, if \(A : \mathbb{R}^d \to \mathbb{R}^m\) 
  is a linear transformation, then \(AX\) is Gaussian with covariance \(AKA^T\) 
  and mean \(Am\).
\end{theorem}
\begin{proof}
  Follows since,
  \[\mathbb{E}e^{i \langle \lambda, AX \rangle} = \mathbb{E}e^{i \langle A^T\lambda, X\rangle}
    = e^{i \langle A^T \lambda, m \rangle - \frac{1}{2}\langle KA^T \lambda, A^T \lambda\rangle}
    = e^{i \langle \lambda, Am \rangle - \frac{1}{2}\langle AKA^T \lambda, \lambda\rangle}.\]
\end{proof}

\begin{proposition}
  Linear combinations of independent Gaussian random variables are also Gaussian.
\end{proposition}
\begin{proof}
  Exercise.
\end{proof}

\begin{definition}[Gaussian Process]
  A stochastic process is Gaussian if its finite dimensional distributions 
  are Gaussian.
\end{definition}

\subsection{Kolmogorov's Extension Theorem}

Let \((x_n)\) be a stochastic process with state space \(\mathcal{X}\). 
Denote 
\[\mathcal{X}^{\mathbb{N}_0} := \prod_{i = 0}^\infty \mathcal{X} = 
  \{(a_0, a_1, \cdots) \mid a_i \in \mathcal{X}\}.\]
We may consider \((x_n)\) as a map from \(\Omega \to \mathcal{X}^{\mathbb{N}_0}\) by 
defining 
\[(x_n) : \Omega \to \mathcal{X}^{\mathbb{N}_0} : \omega \mapsto (x_n(\omega))_{n = 0}^\infty.\]
We would like \((x_n)\) to be measurable as a map from \(\Omega \to \mathcal{X}^{\mathbb{N}_0}\) 
and so, let us first equip \(\mathcal{X}^{\mathbb{N}_0}\) with a \(\sigma\)-algebra.

\begin{definition}
  Given \(\mathcal{X}_i\) complete separable metric spaces for \(i \in \Lambda\), 
  define the projection maps 
  \[\pi_m : \prod_{i \in \Lambda} \mathcal{X}_i \to \mathcal{X}_m : (a_i)_{i \in \Lambda} \mapsto a_m.\]
  Then, we define 
  \(\bigotimes_{i \in \Lambda} \mathcal{B}(\mathcal{X}_i) = 
    \sigma(\pi_i \mid i \in \Lambda).\)

  We note that this definition can be easily extended to arbitrary measurable 
  spaces.
\end{definition}

\begin{proposition}
  If \((x_n)\) is a stochastic process, then as a map from \(\Omega \to \mathcal{X}^{\mathbb{N}_0}\),
  \((x_n)\) is \(\bigotimes_{n} \mathcal{B}(\mathcal{X})\)-measurable.
\end{proposition}

With this in mind, we can push forward the probability measure along a stochastic 
process, inducing a measure on \(\mathcal{X}^{\mathbb{N}_0}\). In particular, 
we have the measure space \((\mathcal{X}^{\mathbb{N}_0}, 
\bigoplus_n \mathcal{B}(\mathcal{X}), (x_n)_* \mathbb{P})\).

On the other hand, if we only consider the first \(n\)-components of the 
process \((x_i)\), by the same argument, \((x_i)_{i = 1}^n\) forms a measurable 
map from \(\Omega \to \mathcal{X}^{n}\). Then, in this case, we call the 
push-forward measure \(\mathcal{L}((x_i)_{i = 1}^n) = {(x_i)_{i = 1}^n}_* \mathbb{P}\) 
the joint distribution. These are known as the finite dimensional 
distributions of \((x_n)\).

\begin{definition}
  Let \((\mu_n)_{n = 1}^\infty\) be a sequence of probability measures on 
  \(\mathcal{X}^n\) (i.e. \(\mu_n\) is aprobability measures on \(\mathcal{X}^n\)). 
  Then \((\mu_n)_{n = 0}^\infty\) is said to satisfy Kolmogorov's consistency 
  condition of 
  \[\mu_{n + 1}(A_1 \times \cdots A_n \times \mathcal{X}) = 
    \mu_n(A_1 \times \cdots \times A_n)\]
  for all \(n \ge 0\), \(A_i \in \mathcal{B}(\mathcal{X})\).
\end{definition}

\begin{theorem}[Kolmogorov's Extension Theorem]
  Let \((\mu_n)_{n = 1}^\infty\) be a sequence of probability measures which are 
  consistent. Then, there exists a unique probability measure \(\mu\) on 
  \(\mathcal{X}^{\mathbb{N}_0}\) such that for any \(n\), 
  \(A \in \bigotimes_{i = 1}^n \mathcal{B}(\mathcal{X})\), 
  \[\mu(A \times \mathcal{X}^{\mathbb{N}_0}) = \mu_n(A).\]
  In other words, if we denote \(\text{pr}_n\) the map 
  \[\text{pr}_n : \mathcal{X}^{\mathbb{N}_0} \to \mathcal{X}^n : 
    (a_i)_{i = 1}^\infty \mapsto (a_i)_{i = 1}^n,\]
  \(\mu\) is the unique measure which satisfies 
  \[(\text{pr}_n)_* \mu = \mu_n.\]
\end{theorem}

\begin{corollary}
  The finite dimensional distributions of a stochastic process \((x_n)\) determines 
  uniquely the probability distribution of the process on \(\mathcal{X}^{\mathbb{N}_0}\).
\end{corollary}

\begin{corollary}
  Given any consistent family of probability measures \((\mu_n)\), there exists a 
  stochastic process with \((\mu_n)\) as its finite dimensional distributions.
\end{corollary}
\begin{proof}
  Kolmogorov's extension theorem implies that there exists a compatible measure 
  \(\mu\) on \(\mathcal{X}^{\mathbb{N}_0}\). Thus, it suffices to find 
  a \(\mathcal{X}^{\mathbb{N}_0}\)-valued random variable with distribution 
  \(\mu\). We will describe a trivial method for this purpose below.
\end{proof}

Let \(\mu\) be a probability measure on \(\mathcal{X}\). Then, setting 
\(\Omega = \mathcal{X}, \mathcal{F} = \mathcal{B}(\mathcal{X})\) and \(\mathbb{P} = \mu\),
it is clear that the push-forward of \(\mathbb{P}\) along the identity map 
provides \(\mu\). Thus, the identity is a random variable with the distribution 
\(\mu\).

Thus, in the case of the corollary above, we set \(\Omega = \mathcal{X}^{\mathbb{N}_0}\),
\(\mathcal{F} = \bigotimes\mathcal{B}(\mathcal{X})\), and \(\mathbb{P} = \mu\). 
We call this probability space the canonical probability space and call the 
resulting process \((\pi_n)\) the canonical process.

\begin{definition}[Shift Operator]
  For \(n \in \mathbb{N}\), we define the \(n\)-th shift operator by 
  \[\theta_n : \mathcal{X}^{\mathbb{N}_0} \to \mathcal{X}^{\mathbb{N}_0} : 
    (a_0, a_1, \cdots) \mapsto (a_n, a_{n + 1}, \cdots).\]
  For connivance, we will denote the above property by \(\theta_n(a.) = (a_{n +}.)\). 
\end{definition}

\begin{definition}[Stationary Process]
  A stochastic process \((x.)\) is stationary if for any \(n\), 
  \[\mathcal{L}(\theta_n x.) = \mathcal{L}(x.).\]
\end{definition}

Straight away, by Kolmogorov's extension, we see that an equivalent definition 
for the stationary process is that the finite dimensional distributions 
of \((\theta_n x.)\) are the same as the finite dimensional distributions of \((x.)\).

In general, a process \((x_n)\) is stationary if 
\[\mathcal{L}(x_n, \cdots, x_{n + m}) = \mathcal{L}(x_0, \cdots, x_m)\]
for all \(n, m \ge 0\). In the case that \((x_n)\) is a Gaussian process, 
as \(\mathcal{L}(x_{i_1} \cdots x_{i_n})\) is determined by 
\((\mathbb{E}x_{i_1}, \cdots, \mathbb{E}x_{i_n})\) and \(\text{Cov}(x_{i_k}, x_{i_l})\),
it is stationary if 
\[\mathbb{E}x_n = \mathbb{E}x_0\]
for all \(n\) and the covariances are shift invariant.

\subsection{Transition Probability}

Given a Markov process \((x_n)\) on the state space \(\mathcal{X}\), for 
\(A \in \mathcal{B}(\mathcal{X})\), the function \(\mathbb{P}(x_{n + 1} \in A \mid x_n)\) 
is a Borel function of \(x_n\). This function might depend on \(A\), 
\(n\) and \(n - 1\). Suppose the case that this function does not depend on time 
(i.e. time homogeneous), that is there exists some function \(\phi(x, A)\) such that 
\[\mathbb{P}(x_{n + 1} \in A \mid x_n) = \phi(x_n, A)\]
almost surely. Fixing \(x\), under some regularities, its not difficult to show that 
\(\phi(x, \cdot)\) form a probability measure. We shall assume this.

\begin{definition}
  The set \(P := \{P(x, A) \mid x \in \mathcal{X}, A \in \mathcal{B}(\mathcal{X})\}\)
  is said to be a family of transition probabilities if
  \begin{itemize}
    \item for all \(x \in \mathcal{X}\), \(P(x, \cdot)\) is a probability 
      measure;
    \item for any \(A \in \mathcal{B}(\mathcal{X})\), \(P(\cdot, A)\) is Borel 
      measurable.
  \end{itemize} 
\end{definition}

As an example, consider the Markov chain \((x_n)\) on \(\mathcal{X}\) where 
\(x_{n + 1} := F(x_n, \xi_{n + 1})\) for \(x_0, \xi_1, \xi_2, \cdots\)
independent with \((\xi_i : \Omega \to \mathcal{Y}) \sim \mu\) for all \(i\). 
Then, for all \(\omega \in \Omega\), we have (recall that we denote the event 
\(x_n^{-1}(\{\omega\})\) by \(x_n = \omega\)
\[\begin{split}
  \mathbb{P}(x_{n + 1} \in A \mid x_n = \omega) & = \mathbb{P}(F(x_n(\omega), \xi_{n + 1}) \in A)\\
  & = \int_{\mathcal{Y}} \mathbf{1}_A(F(x_n(\omega), \xi_{n + 1})) \dd \mathbb{P}\\
  & = \int_{\mathcal{Y}} \mathbf{1}_A(F(x_n(\omega), y)) \mu(\dd y).
\end{split}\]
Hence, defining 
\[P(x, A) := \int_{\mathcal{Y}} \mathbf{1}_A(F(x, y)) \mu(\dd y),\]
\(\{P(x, A)\}\) are the transition probabilities and 
\[\mathbb{P}(x_{n + 1} \in A \mid x_n = \omega) = P(x_n(\omega), A).\]

We remark that, in the case that the state space is countable, by \(\sigma\)-additivity,
it is sufficient to work with transitional probabilities of singletons. In particular, 
the transitional probability is simply determined by 
\[P(i, j) = P(i, \{j\}), i, j \in \mathcal{X}.\]

\begin{proposition}
  Let \((x_n)\) be a Markov chain such that there exists a transition 
  probability \(P\) for which 
  \[\mathbb{P}(x_{n + 1} \in A \mid x_n) = P(x_n, A)\]
  for all \(A \in \mathcal{B}(\mathcal{X})\). Then, for all \(f \in \mathcal{B}_b(\mathcal{X})\),
  \[\mathbb{E}(f(x_{n + 1}) \mid x_n) = \int_{\mathcal{X}} f(y) P(x_n, \dd y).\]
\end{proposition}
\begin{proof}
  Choosing \(f = \mathbf{1}_A\), we see that the property is true for simple functions 
  and so, the results can be extended to all functions by the monotone convergence theorem.
\end{proof}

\begin{proposition}
  Let \((x_n)\) as defined above, for all \(f \in \mathcal{B}_b(\mathcal{X})\), 
  we have 
  \[\mathbb{P}(x_{n + 2} \in A \mid x_n) = \int_{\mathcal{X}}P(y, A)P(x_n, \dd y).\]
  Thus, we define the two-step transition probability by 
  \[P^2(x, A) = \int_{\mathcal{X}} P(y, A) P(x, \dd y),\]
  such that \(\mathbb{P}(x_{n + 2} \in A \mid x_n) = P^2(x_n, A)\) almost surely.
\end{proposition}
\begin{proof}
  By the tower law, we have 
  \[\begin{split}
    \mathbb{P}(x_{n + 2} \in A \mid x_n) & = \mathbb{E}(\mathbb{E}(\mathbf{1}_A(x_{n + 2}) \mid x_{n + 1}, x_n) \mid x_n)\\
    & = \mathbb{E}(\mathbb{E}(\mathbf{1}_A(x_{n + 2}) \mid x_{n + 1}) \mid x_n)\\
    & = \mathbb{E}(P(x_{n + 1}, A) \mid x_n)\\
    & = \int_{\mathcal{X}}P(y, A)P(x_n, \dd y),
  \end{split}\]
  where the last equality follows by the above proposition.
\end{proof}

The above process can be extended to \(k\)-steps by induction. In particular, 
for all \(k \in \mathbb{N}\), we have 
\[\mathbb{P}(x_{n + (k + 1)} \in A) \mid x_n) = 
  \int_{\mathcal{X}} P^i(y, A) P^j(x_n, \dd y)\]
for any \(i, j \in \mathbb{N}, i + j = k\). This is known as the Chapman-Kolmogorov 
equation.

\begin{definition}
  A family 
  \[\{P^n(x, \cdot) \mid x\in \mathcal{X}, n \in \mathbb{N}_0\}\]
  is said to be a transition function if 
  \begin{itemize}
    \item \(P^n(x, \cdot)\) is a transition probability for any \(x, n\);
    \item \(P^0(x, \cdot) = \delta_x\) for any \(x\);
    \item (Chapman-Kolmogorov) for all \(n, m \ge 0, x \in \mathcal{X}, A \in \mathcal{B}(\mathcal{B})\),
    \[P^{n + m}(x, A) = \int_{\mathcal{X}} P^n(y, A) P^m(x, \dd y).\]
  \end{itemize}
\end{definition}

\begin{proposition}
  The Chapman-Kolmogorov equation is satisfied if and only if for all 
  \(f \in \mathcal{B}_b(\mathcal{X}), n, m \ge 0, x \in \mathcal{X}, A \in \mathcal{B}(\mathcal{B})\), 
  \[\int_{\mathcal{X}}f(y)P^{n + m}(x, \dd y) = 
  \int_{\mathcal{X}}\left(\int_{\mathcal{X}} f(z) P^n(y, \dd z)\right) P^m(x, \dd y).\]
\end{proposition}
\begin{proof}
  Exercise.
\end{proof}

As alluded to above, the \(k\)-step transitional probabilities can be constructed from a 
1-step transitional probabilities. In particular, given the 1-step transitional 
probability \(P\),
\begin{enumerate}
  \item set \(P^0(x, \cdot) = \delta_x\);
  \item set \(P^1(x, \cdot) = P(x, \cdot)\);
  \item for all \(n > 1\), \(x \in \mathcal{X}\), for all \(A \in \mathcal{B}(\mathcal{X})\), set 
    \[P^{n + 1}(x, A) := \int_{\mathcal{X}} P(y, A) P^n(x, \dd y).\]
\end{enumerate}
It remains to show that this construction satisfies the Chapman-Kolmogorov equation.
Indeed, by induction, suppose that the Chapman-Kolmogorov equation is satisfied for 
all \(k \le n + m\), then 
\[\begin{split}
  P^{n + m + 1}(x, A) & = \int_{\mathcal{X}}P(z, A)P^{n + m}(x, \dd z) \\
  & = \int_{\mathcal{X}}\left(\int_{\mathcal{X}} P(z, A) P^j(y, \dd y)\right) P^{n + m - j}(x, \dd y)\\
  & = \int_{\mathcal{X}}P^{j + 1}(y, A)P^{n + m - j}(x, \dd y)
\end{split}\]
for all \(j = 0, 1, \cdots\), where the second and third equality follows by the 
inductive hypothesis and Fubini's theorem.

\begin{definition}
  A transition probability \(P\) is said to  be the transitional probability of 
  the Markov chain \((x_n)\) if 
  \[\mathbb{P}(x_{n + 1} \in A \mid x_n) = P(x_n, A)\]
  almost surely for all \(A \in \mathcal{B}(\mathcal{X})\) and any \(n \ge 0\).

  If a Markov chain has a transitional probability \(P\), then we say the Markov 
  chain is time homogeneous.
\end{definition}

From this point forward, unless otherwise stated, we will assume our Markov chains 
to be time homogeneous.

\begin{theorem}
  Let \((x_n)\) be a Markov process with transition probability \(P\). Then 
  \begin{itemize}
    \item \(\mathbb{P}(x_{n + m} \in A \mid x_m) = P^n(x_m, A)\) almost surely for any 
      \(n, m \ge 0, A \in \mathcal{B}(\mathcal{X})\);
    \item if \(\mathcal{L}(x_0) = \mu\), then 
      \[\mathbb{P}(x_n \in A) = \int_{\mathcal{X}} P^n(x, A) \mu(\dd x).\] 
  \end{itemize}
\end{theorem}
\begin{proof}
  The first property follows by induction. Indeed, if
  for some \(k \in \mathbb{N}\), \(\mathbb{P}(x_{k + m} \in A \mid x_m) = P^k(x_m, A)\), 
  for any \(m\), then 
  \[\begin{split}
    \mathbb{P}(x_{k + 1 + m} \in A \mid x_m) 
    & = \mathbb{E}(\mathbb{P}(x_{k + 1 + m} \in A \mid \mathcal{F}_{k + m}) \mid x_m) \\
    & = \mathbb{E}(P(x_{m + k}, A) \mid x_m) \\
    & = \int_{\mathcal{X}}P(z, A) P^k(x_m \dd z) = P^{k + 1}(x_m, A)
  \end{split}\]
  where the second to last equality follows as \(\mathbb{E}(f(x_{k + m}) \mid x_m) = 
  \int f(z) P^k(x_m, \dd z)\).

  The second property follows as 
  \[\begin{split}
    \mathbb{P}(x_n \in A) = \mathbb{E}(\mathbb{P}(x_n \in A \mid x_0)) 
    = \mathbb{E}(P^n(x_0, A)) = \int_{\mathcal{X}}P^n(y, A) \mu(\dd y)
  \end{split}\]
  as required.
\end{proof}

\begin{theorem}[Einstein's Relation]
  If \((x_n)\) is a Markov chain with transition probability \(P\) and initial 
  distribution \(\mu\), then, its finite dimensional distributions are 
  \[\mathbb{P}(x_0 \in A_0, \cdots, x_n \in A_n) = 
    \int_{A_0} \cdots \int_{A_{n - 1}} P(y_{n - 1}, A_n) P(y_{n - 2}, \dd y_{n - 1}) \cdots P(y_0, \dd y_1) \mu(\dd y_0).\]
\end{theorem}

We remark that the above definition provides a sequence of consistent measures.
Namely, if we define 
\[\mu(A_0 \times \cdots \times A_n) := 
  \int_{A_0} \cdots \int_{A_{n - 1}} P(y_{n - 1}, A_n) P(y_{n - 2}, \dd y_{n - 1}) \cdots P(y_0, \dd y_1) \mu(\dd y_0),\]
the sequence of measures \((\mu_n)\) is consistent.
\begin{proof}
  For \(A_0, A_1 \in \mathcal{B}(\mathcal{X})\), we observe 
  \[\begin{split}
    \mathbb{P}(x_0 \in A_0, x_1 \in A_1) 
    & = \mathbb{E}(\mathbb{E}(\mathbf{1}_{A_0}(x_0) \mathbf{1}_{A_1}(x_1) \mid x_0)) \\
    & = \mathbb{E}(\mathbf{1}_{A_0}(x_0)\mathbb{E}(\mathbf{1}_{A_1}(x_1) \mid x_0)) \\
    & = \mathbb{E}(\mathbf{1}_{A_0}(x_0) P(x_0, A_1))\\
    & = \int_{A_0} P(y_0, A_1) \mu(\dd y_0).
  \end{split}\]
  Hence, by induction, the relation follows.
\end{proof}

\begin{theorem}
  If \((x_n)\) is a process satisfying Einstein's relation, then it is a 
  Markov chain with transition probability \(P\).
\end{theorem}
\begin{proof}
  Einstein's relation can be extended to all bounded measurable functions through 
  the usual process with the monotone convergence theorem and thus, we have 
  for any \(f_i \in \mathcal{B}_b(\mathcal{X})\), 
  \[\begin{split}
    \mathbb{E}\left(\prod_{i = 0}^n f_i(x_i)\right) 
    & = \int \cdots \int \prod_{i = 0}^n f_i(y_i) P(y_{n - 1}, \dd y_{n}) \cdots P(y_0, \dd y_1) \mu(\dd y_0).
  \end{split}\]
  By Fubini's theorem, the above becomes
  \[\begin{split}
    \cdots & = \int f_n(y_n)P(y_{n - 1}, \dd y_n)
      \int \cdots \int \prod_{i = 0}^{n - 1} f_i(y_i) \prod_{i = 1}^{n - 1} P(y_i, \dd y_{i + 1}) \mu(\dd y_0)\\
    & = \mathbb{E}(f_n(x_n) \mid x_{n - 1} = y_{n - 1}) 
      \int \cdots \int \prod_{i = 0}^{n - 1}f_i(y_i) \prod_{i = 1}^{n - 1} P(y_i \dd y_{i + 1}) \mu(\dd y_0)\\
    & = \mathbb{E}\left(\mathbb{E}(f_n(x_n) \mid x_{n - 1}) \prod_{i = 0}^{n - 1}f_i(x_i)\right)
  \end{split}\]
  which is equivalent to the Markov property.
\end{proof}

\begin{theorem}[Existence of Markov Chain]
  Given a family of transition probabilities on \(P\) on \(\mathcal{X}\) and any 
  probability measure \(\mu_0\) on \(\mathcal{X}\), there exists a unique (in distribution) 
  Markov process \(x\) with transition probability \(P\) and initial distribution 
  \(\mu_0\). 
\end{theorem}
\begin{proof}
  Define \(\mu_n\) on \(\mathcal{X}^{n + 1}\) such that 
  \[\mu_n(A_0 \times A_n) := \int_{A_0} \cdots \int_{A_{n - 1}} 
  P(y_{n - 1}, A_n) P(y_{n - 2}, \dd y_{n - 1}) \cdots P(y_1, \dd y_0) \mu_0(\dd y_0).\]
  It is routine to check that this sequence of measures is well-defined and consistent, and 
  thus, by the Kolmogorov extension theorem, there exists a unique \(P_\mu\) on \(\mathcal{X}^\infty\) 
  such that \(P_\mu\) projects to \(\mu_n\) on \(\mathcal{X}^{n + 1}\). Thus, 
  taking \((\pi_n)\) to be the canonical process on the canonical space 
  \((\mathcal{X}^\infty, \otimes \mathcal{B}(\mathcal{X}), P_\mu)\), we have 
  found a Markov process which satisfies the condition.
\end{proof}

Consider again the case where the state space is countable \(\mathcal{X} = \mathbb{N}\). 
As mentioned previously, the transition probability on \(\mathcal{X}\) is then 
determined by \(p_{ij} = P(i, \{j\})\). As \(P(i, \cdot)\) is a probability 
measure by definition, 
\[1 = P(i, \mathcal{X}) = \sum_{j \in \mathcal{X}} p_{ij}.\]
In the case that \(\mathcal{X}\) is finite, these \(p_{ij}\) can be represented 
as a matrix, motivating the definition of a stochastic matrix.

\begin{definition}[Stochastic Matrix]
  A matrix \(p = (p_{ij})\) with \(p_{ij} \ge 0\) is said to be a stochastic 
  matrix if \(\sum_{j \in \mathcal{X}} p_{ij} = 1\).
\end{definition}

In the discrete case, our construction of the transition probability from 
the 1-step transition probability is straightforward. In particular, we obtain
\[P^{n + 1}(i, A) = \int_{\mathcal{X}} P(y, A) P^n(i, \dd y) = \sum_{k \in \mathcal{X}} P(k, A) P^n(i, k).\]
Thus, if we write \(P(i, \{j\}) = p_{ij}\), then
\[P^2(i, \{j\}) = \sum_{k \in \mathcal{X}} p_{ik}p_{kj} = ((p_{kl})_{k,l \in \mathcal{X}}^2)_{ij},\]
where the last term denotes matrix multiplication. Thus, by induction, we obtain that 
\[P^n(i, \{j\}) = \sum_{k_1 \in \mathcal{X}} \cdots \sum_{k_{n - 1} \in \mathcal{X}}
  p_{ik_1} p_{k_1k_2} \cdots p_{k_{n-1}j} = ((p_{kl})_{k,l \in \mathcal{X}}^n)_{ij}.\]

\subsection{Transition Operator}

In the case of the transition probability \(P\) of a Markov chain \((x_n)\), we have the 
relation 
\[\mathbb{P}(x_{n + 1} \in A) = \int_{\mathcal{X}} P(y, A) \mu_n(\dd y)\]
where \(\mu_n = \mathcal{L}(x_n)\). Thus, in some sense, the transitional probability 
the an operator on measures changing the distribution to the next time step. 
This motivates the following definition.

\begin{definition}[Transition Operator]
  The transition operator \(T^*\) given the transition probability \(P\) on the 
  set of probability measures on \(\mathcal{X}\) is defined to be 
  \[T^* \mu(A) := \int_{\mathcal{X}} P(y, A) \mu(\dd y)\]
  for all \(A \in \mathcal{B}(\mathcal{X})\).
\end{definition}

With this definition, we obtain that, given a Markov process \((x_n)\) with the 
transition probability \(P\), we have \((T^*)^n(\mathcal{L}(x_m)) = \mathcal{L}(x_{n + m})\).  

\begin{definition}[Dual Transition Operator]
  The dual transition operator \(T_*\) given the transition probability \(P\) is 
  defined to be an operator acting on \(\mathcal{B}_b(\mathcal{X})\) such that 
  \[T_* f(x) = \int_{\mathcal{X}} f(y) P(x, \dd y)\]
  for all \(f \in \mathcal{B}_b(\mathcal{X})\).
\end{definition}

Equivalently, the dual transition operator acting on \(f\) is 
\[T_* f(x) = \mathbb{E}(f(x_1) \mid x_0 = x)\]
where \((x_n)\) is the Markov process associated with \(P\).

\begin{proposition}
  The above operators are dual in the sense that, for all \(f \in \mathcal{B}_b(\mathcal{X})\),
  \[\int_{\mathcal{X}} T_* f \dd \mu = \int_{\mathcal{X}} f \dd(T^* \mu).\]
\end{proposition}
\begin{proof}
  Hint: first prove for \(f = \mathbf{1}_A\).
\end{proof}

For simplicity, we will denote both \(T^*\) and \(T_*\) with \(T\) when there is no 
confusion.

We note that \(T^*\) extends to signed measures allowing us to show linearity 
(recall that the set of signed measures form a vector space over \(\mathbb{R}\)).

In the case that \(\mathcal{X} = \{1, \cdots, N\}\) is finite, a probability 
measure \(\nu\) is uniquely determined by its values on singletons 
\(\{\nu(\{1\}), \cdots, \nu(\{N\})\}\). Then, if \(P = (p_{ij})\) is a stochastic matrix,
we have \(T\nu = (\nu(\{i\}))_{i = 1}^N P\).

\newpage 

\section{Strong Markov Process}

We will in the section continue to let \((\Omega, \mathcal{F}, \mathbb{P})\) be a 
probability space and let \((\mathcal{F}_n)\) be a filtration on this probability 
space.

\subsection{Stopping Times}

\begin{definition}[Stopping Time]
  A function \(T : \Omega \to \overline{\mathbb{N}} := \{0, 1, \cdots \} \cup \{\infty\}\) 
  is said to be a stopping time with respect to the filtration \((\mathcal{F}_n)\) if 
  \[\{\omega \mid T(\omega) = n\} \in \mathcal{F}_n\]
  for all \(n \ge 0\).
\end{definition}

This definition can be easily generalized to continuous time with the codomain 
being \(\overline{\mathbb{R}}_+\) and taking 
\[\{\omega \mid T(\omega) \le t\} \in \mathcal{F}_t\]
for all \(t \ge 0\) instead. The generalized definition is consistent with the 
discrete version since \(\{T \le n\} = \bigcup_{k = 1}^n {T = k}\) and 
\(\{T = n\} = \{T \le n + 1\} \setminus \{T \le n\}\) and thus, 
\(T\) is an \((\mathcal{F}_n)\) stopping time if and only if \(\{T \le n\} \in \mathcal{F}_n\) 
for all \(n \ge 0\).

\begin{definition}[Stopped Process]
  Let \(T\) be a stopping time and let \((x_n)\) be a stochastic process. We define 
  the stopped process \((x^T_n)\) by 
  \[x^T_n(\omega) := x_{\min \{n, T(\omega)\}}(\omega),\]
  for all \(\omega \in \Omega\), \(n \in \overline{\mathbb{N}}\).
\end{definition}

In some sense, the stopped process as the name suggests, stop the process once some 
condition has been achieved. Consider the random walk on the integers with the 
stopping time being the walk reaches 4. Then, for each \(\omega \in \Omega\), 
the stopped process is the same as the process as long as \(x_n(\omega) \le 4\) 
while after \(x_k(\omega) = 4\), the process stops in the sense that 
\(x^T_n(\omega)\) is constant for all \(n \ge k\).

The following three propositions are exercises.

\begin{proposition}
  If \(S, T\) are stopping times, then so are 
  \[S \wedge T := \min\{S, T\} \text{ and } S \vee T := \max\{S, T\}.\]
\end{proposition}

\begin{proposition}
  If \((S_n)\) is a sequence of stopping times, then 
  \[\limsup_{n \to \infty} S_n \text{ and } \liminf_{n \to \infty} S_n\]
  are stopping times.
\end{proposition}

\begin{proposition}
  Constant functions are stopping times.
\end{proposition}

\begin{definition}[Stopped \(\sigma\)-algebra]
  Given a stopping time \(T\), let \(\mathcal{F}_\infty := \bigvee_{n = 0}^\infty \mathcal{F}_n\) 
  and define 
  \[\mathcal{F}_T := \{A \in \mathcal{F}_\infty \mid A \cap \{T = n\} \in \mathcal{F}_n, \forall n \ge 0\}.\]
  For the continuous case, the set \(\{T = n\}\) is replaced by \(\{T \le t\}\).
\end{definition}

We note that in the case that \(T\) is a constant, \(\mathcal{F}_T = \mathcal{F}_m\).
Furthermore, if \(S \le T\) a.e. then \(\mathcal{F}_S \subseteq \mathcal{F}_T\) 
(we assume the probability space is complete, i.e. null-sets are measurable).

\begin{proposition}
  For a stopping time \(T < \infty\), the stopped \(\sigma\)-algebra can be 
  equivalently defined as
  \[\mathcal{F}_T = \{A \in \mathcal{F} \mid A \cap \{T \le t\} \in \mathcal{F}_t, \forall t \ge 0\}.\]
\end{proposition}
\begin{proof}
  Clearly the right hand side is larger and so, it suffices to show that, for all 
  \(A \in \mathcal{F}\) such that \(A \cap \{T \le t\} \in \mathcal{F}_t, \forall t \ge 0\),
  \(A \in \mathcal{F}_T\). Now, by considering 
  \[A = \bigcup_{n \in \mathbb{N}} A \cap \{T \le n\},\]
  where \(A \cap \{T \le n\} \in \mathcal{F}_n\), we have 
  \[A = \bigcup_{n \in \mathbb{N}} A \cap \{T \le n\} \in \bigvee_{n = 0}^\infty \mathcal{F}_n,\]
  and hence, \(A \in \mathcal{F}_T\) as required.
\end{proof}

\begin{proposition}
  \(T\) is \(\mathcal{F}_T\)-measurable.
\end{proposition}
\begin{proof}
  Let \(A = \{T = m\}\) and by construction, for all \(n \ge 0\), we have 
  \(A \cap {T = n}\) is \(\varnothing\) if \(n \neq m\) and \(\{T = n\}\) if 
  \(n = m\) both contained in \(\mathcal{F}_n\). Thus, \(\{T = m\} \in \mathcal{F}_T\) 
  and hence \(T\) is \(\mathcal{F}_T\)-measurable.
\end{proof}

\begin{proposition}
  For a stopping time \(T < \infty\), and an adapted process \((x_n)\), 
  \begin{itemize}
    \item \(\omega \mapsto x_{T(\omega)}(\omega)\) (denoted by \(x_T\)) is \(\mathcal{F}_T\)-measurable; 
    \item for all \(n\), \(x_n^T\) is \(\mathcal{F}_T\)-measurable. 
  \end{itemize}
\end{proposition}
\begin{proof}
  We have \(\{x_T \in A\} \cap \{T = n\} = \{x_n \in A\} \cap \{T = n\} \in \mathcal{F}_n\) 
  as \((x_n)\) is adapted. Thus, \(x_T\) is \(\mathcal{F}_T\)-measurable.

  The second property follows as \(x^T_n = x_{T \wedge n}\) where \(T \wedge n\) 
  is a stopping time. Thus, \(x_{T \wedge n}\) is \(\mathcal{F}_{T \wedge n}\)-measurable. 
  On the other hand, as \(T \wedge n \le T\), we have \(\mathcal{F}_{T \wedge n} \subseteq \mathcal{F}_T\)
  and thus, \(x^T_n\) is \(\mathcal{F}_T\)-measurable as required.
\end{proof}

\begin{proposition}
  If \(T < \infty\) is a \(\mathcal{F}_T^x\)-stopping time, then for all \(n \ge 0\), 
  \[\{T = n\} \in \sigma(x_{T \wedge 0}, \cdots, x_{T \wedge n}).\]
  That is to say \(T\) is a stopping time with respect to the natural filtration of 
  stopped process \(x_n^T\).
\end{proposition}
\begin{proof}
  It suffices to show \(\mathbf{1}_{T = n}\) is of the form \(\phi(x_{T \wedge 0}, \cdots, x_{T \wedge n})\)
  for some \(\phi \in \mathcal{B}_b(\mathcal{X}^{n + 1})\). We will prove this by induction.

  Suppose there exists some \(\phi_l \in \mathcal{B}_b(\mathcal{X}^{l + 1})\) for all 
  \(l \le k - 1\) such that 
  \[\mathbf{1}_{\{T = l\}} = \phi(x_{T \wedge 0}, \cdots, x_{T \wedge l}).\]
  We observe (factorisation lemma implies the existence of \(\psi\)),
  \[\begin{split}
    \mathbf{1}_{\{T = k\}} & = \mathbf{1}_{\{T = k\}}\mathbf{1}_{\{T \ge k\}} = \psi(x_0, \cdots, x_k) \mathbf{1}_{\{T \ge k\}}\\
    & = \psi(x_{T \wedge 0}, \cdots, x_{T \wedge k}) \mathbf{1}_{\{T \ge k\}} \\ 
    & = \psi(x_{T \wedge 0}, \cdots, x_{T \wedge k}) (1 - \mathbf{1}_{\{T \le k - 1\}}).\\
  \end{split}\]
  Now, since \(1 - \mathbf{1}_{\{T \le k - 1\}} = 1 - \sum_{l \le k - 1} \phi_l\), the result follows.
\end{proof}

\begin{proposition}
  Let us denote \(\sigma(x^T) := \sigma(x_{T \wedge n} \mid n)\), then if 
  \(T < \infty\) is a stopping time with respect to \(\mathcal{F}_n^x\), 
  \[\mathcal{F}_T = \sigma(x^T).\] 
\end{proposition}
\begin{proof}
  Clearly \(\mathcal{F}_T \supseteq \sigma(x^T)\) so we will prove the reverse. 
  Let \(A \in \mathcal{F}_T\), then, for all \(n \ge 0\), as \(A \cap \{T = n\}\) 
  is \(\mathcal{F}_n\) measurable, by the factorisation lemma, there exists some 
  \(\psi \in \mathcal{B}_n(\mathcal{X}^{n + 1})\) such that
  \[\psi(x_0, \cdots, x_n) = \mathbf{1}_{A \cap \{T = n\}} = \mathbf{1}_A\mathbf{1}_{\{T = n\}}.\]
  So,
  \[\mathbf{1}_A\mathbf{1}_{\{T = n\}} =  \mathbf{1}_A \mathbf{1}_{\{T = n\}}^2 = 
    \mathbf{1}_{\{T = n\}} \psi(x_0, \cdots, x_n) = 
    \mathbf{1}_{\{T = n\}} \psi(x_{T \wedge 0}, \cdots, x_{T \wedge n}).\]
  Thus, as \(\{T = n\} \in \sigma(x_{T \wedge 0}, \cdots, x_{T \wedge n}) \subseteq \sigma(x^T)\) by the 
  above lemma, \(\mathbf{1}_A \mathbf{1}_{\{T = n\}}\) is \(\sigma(x^T)\)-measurable. Hence,
  \[\mathbf{1}_A = \sum_{n = 0}^\infty \mathbf{1}_A \mathbf{1}_{\{T = n\}} 
    \in \sigma(x^T)\]
\end{proof}

\subsection{Strong Markov Property}

Recall the shift operator, we define the following operator.

\begin{definition}
  Given a stopping time \(T < \infty\) a.e. we define \(\theta_T\) 
  such that for all stochastic process \(x. = (x_n)\) 
  \[(\theta_T x.)_n(\omega) := x_{T(\omega) + n}(\omega).\]
\end{definition}

\begin{definition}[Strong Markov Property]
  A stochastic process \(x.\) is said to have the strong Markov property if 
  for every stopping time \(T < \infty\) a.e. and every 
  \(\Phi \in \mathcal{B}_b(\mathcal{X}^\infty)\), we have 
  \[\mathbb{E}(\Phi(\theta_T x.) \mid \mathcal{F}_T) = \mathbb{E}(\Phi(\theta_T x.) \mid x_T),\]
  almost everywhere.
\end{definition}

We may assume that \(\Phi\) is independent in its components. Namely, the strong Markov 
property is equivalent to 
\[\mathbb{E}\left(\prod_{i = 1}^m f_i(\theta_T x.)_{n_i} \mid \mathcal{F}_T\right) = 
  \mathbb{E}\left(\prod_{i = 1}^m f_i(\theta_T x.)_{n_i} \mid x_T\right),\]
for some \(f_i \in \mathcal{B}_b(\mathcal{X})\) and \(n_1 < n_2 < \cdots < n_m\).

Our goal is to show that if \((x_n)\) is a time homogeneous Markov process with 
transition probability \(P\), then it has the strong Markov property.

\begin{proposition}
  Let \(T < \infty\) a.e. be a stopping time. Then 
  \[\mathbb{P}(x_{n + T} \in A \mid \mathcal{F}_T) = P^n(x_T, A)\]
  almost everywhere. In particular, \((x_{n + T})= \theta_T x.\) is a Markov process 
  with transition probability \(P\).
\end{proposition}
\begin{proof}
  Let \(f \in \mathcal{B}_b(\mathcal{X})\), then, as \(\{T = \infty\}\) has measure 0, 
  for all \(B \in \mathcal{F}_T\),
  \[\begin{split}
    \int_B f(x_{n + T}) \dd \mathbb{P} & = \sum_{m = 0}^\infty \int_{B \cap \{T = m\}} f(x_{n + m}) \dd \mathbb{P} \\
    & = \sum_{m = 0}^\infty \int_{B \cap \{T = m\}} \mathbb{E}(f(x_{n + m}) \mid \mathcal{F}_m) \dd \mathbb{P}\\
    & = \sum_{m = 0}^\infty \int_{B \cap \{T = m\}}  \int f(y)P^n(x_m, \dd y) \dd \mathbb{P}\\
    & = \sum_{m = 0}^\infty \int_{B \cap \{T = m\}}  \int f(y)P^n(x_T, \dd y) \dd \mathbb{P}\\
    & = \int_B \int f(y) P^n(x_T, \dd y) \dd \mathbb{P},
  \end{split}\]
  where the second equality follows as \(B \cap \{T = m\}\) is \(\mathcal{F}_m\)-measurable 
  by the definition of stopping time while the third equality follows the the property 
  of the transition probability. Thus, 
  \[\mathbb{E}(f(x_{n + T}) \mid \mathcal{F}_T) = \int f(y) P^n(x_T, \dd y).\]
  Hence, choosing \(f = \mathbf{1}_A\) completes the proof.
\end{proof}


\end{document}
